---
title: "AI Evaluation Workflow Implementation"
tags: ['AI', 'Evaluation', 'Openai', 'Workflow', 'Automation', 'Schema']
created: 2024-10-04
publish: true
---

## üìÖ 2024-10-04 ‚Äî Session: AI Evaluation Workflow Implementation

**üïí 22:45‚Äì23:55**  
**üè∑Ô∏è Labels**: AI, Evaluation, Openai, Workflow, Automation, Schema  
**üìÇ Project**: Dev  
**‚≠ê Priority**: MEDIUM  


### Session Goal
The goal of this session was to design and implement an [[AI]] workflow for evaluating notebooks using predefined rubrics. This involved integrating the OpenAI [[API]] and ensuring proper error handling and schema validation.

### Key Activities
- Designed a framework for an [[AI]] evaluation workflow using traffic light evaluations and structured data storage.
- Updated the `AIEvaluator` class to integrate with the OpenAI [[API]], improving error handling and [[JSON]] result storage.
- Developed a [[JSON]] schema for rubric evaluations, categorizing results as 'green', 'yellow', or 'red'.
- Implemented a [[Python]] function to extract specific consigna schemas from a rubric evaluation schema.
- Resolved errors related to invalid [[JSON]] schema in OpenAI [[API]] calls, ensuring proper structure and validation.

### Achievements
- Successfully designed and partially implemented an [[AI]] evaluation workflow.
- Improved the `AIEvaluator` class for better [[API]] integration and error handling.
- Created a robust [[JSON]] schema for rubric evaluations.

### Pending Tasks
- Complete the integration of the [[AI]] evaluation workflow with the OpenAI [[API]].
- Further test the error handling mechanisms and schema validations.
